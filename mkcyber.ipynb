{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Sample your data\n",
    "def sample_data(df, n_samples=15000):\n",
    "    # Shuffle the data for randomness\n",
    "    df = shuffle(df, random_state=42)\n",
    "    # Sample `n_samples` rows\n",
    "    return df.head(n_samples)\n",
    "\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_data(df, tokenizer, is_training=True, le=None):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['crimeaditionalinfo'].apply(process_text)\n",
    "\n",
    "    if is_training:\n",
    "        le = LabelEncoder()\n",
    "        labels = le.fit_transform(df['label'])\n",
    "        return df['text'].tolist(), labels, le\n",
    "    else:\n",
    "        labels = le.transform(df['label'])\n",
    "        return df['text'].tolist(), labels\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Train Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Val Accuracy: {correct/total:.4f}\\n')\n",
    "\n",
    "# Main execution\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Prepare training data\n",
    "train_texts, train_labels, le = prepare_data(sample_data(df), tokenizer, is_training=True)\n",
    "test_texts, test_labels = prepare_data(sample_data(test_df,n_samples=500), tokenizer, is_training=False, le=le)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CrimeDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = CrimeDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Prepare model\n",
    "num_labels = len(np.unique(train_labels))\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Prepare optimizer and loss\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "def preprocess_text(text, tokenizer, max_length=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "def classify_text(text, model, tokenizer, device):\n",
    "    input_ids, attention_mask = preprocess_text(text, tokenizer)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    return predicted_class, probabilities.cpu().numpy()\n",
    "input_text = (\"I had continue received random calls and abusive messages in my \"\n",
    "              \"whatsapp Someone added my number in a unknown facebook group name \"\n",
    "              \"with Only Girls and still getting calls from unknown numbers pls \"\n",
    "              \"help me and sort out the issue as soon as possible Thank you\")\n",
    "\n",
    "predicted_class, probabilities = classify_text(input_text, model, tokenizer, device)\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Class Probabilities: {probabilities}\")\n",
    "predicted_label = le.inverse_transform([predicted_class])[0]\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
